# tiny_gpt.py
# Minimal GPT-like model for educational purposes.
# Requirements: torch (tested with torch>=1.8)
# Run: python tiny_gpt.py

import math
import torch
import torch.nn as nn
import torch.nn.functional as F

# -------------------------
# Config
# -------------------------
vocab_size = 2000        # small toy vocab
max_seq_len = 64
d_model = 128
n_heads = 4
n_layers = 4
d_ff = 512
dropout = 0.1
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# -------------------------
# Positional encoding (learned)
# -------------------------
class PositionalEmbedding(nn.Module):
    def __init__(self, max_len, dim):
        super().__init__()
        self.pos_emb = nn.Embedding(max_len, dim)
    def forward(self, seq_len):
        # returns (1, seq_len, dim)
        positions = torch.arange(seq_len, device=device).unsqueeze(0)
        return self.pos_emb(positions)

# -------------------------
# Causal (autoregressive) self-attention block
# -------------------------
class CausalSelfAttention(nn.Module):
    def __init__(self, dim, n_heads, dropout):
        super().__init__()
        assert dim % n_heads == 0
        self.n_heads = n_heads
        self.head_dim = dim // n_heads

        self.qkv = nn.Linear(dim, dim * 3, bias=False)
        self.out = nn.Linear(dim, dim)
        self.attn_dropout = nn.Dropout(dropout)
        self.proj_dropout = nn.Dropout(dropout)

    def forward(self, x, mask=None):
        # x: (B, T, D)
        B, T, D = x.size()
        qkv = self.qkv(x)  # (B, T, 3D)
        qkv = qkv.reshape(B, T, 3, self.n_heads, self.head_dim)
        q, k, v = qkv.unbind(dim=2)  # each -> (B, T, H, head_dim)
        # transpose to (B, H, T, head_dim)
        q = q.permute(0, 2, 1, 3)
        k = k.permute(0, 2, 1, 3)
        v = v.permute(0, 2, 1, 3)

        # scaled dot-product attention
        scores = (q @ k.transpose(-2, -1)) / math.sqrt(self.head_dim)  # (B, H, T, T)

        # Causal mask: prevent attending to future positions
        causal_mask = torch.tril(torch.ones(T, T, device=x.device)).unsqueeze(0).unsqueeze(0)  # (1,1,T,T)
        scores = scores.masked_fill(causal_mask == 0, float('-inf'))

        attn = F.softmax(scores, dim=-1)
        attn = self.attn_dropout(attn)

        out = attn @ v  # (B, H, T, head_dim)
        out = out.permute(0, 2, 1, 3).contiguous().reshape(B, T, D)  # (B, T, D)

        out = self.out(out)
        out = self.proj_dropout(out)
        return out

# -------------------------
# Transformer block
# -------------------------
class TransformerBlock(nn.Module):
    def __init__(self, dim, n_heads, d_ff, dropout):
        super().__init__()
        self.ln1 = nn.LayerNorm(dim)
        self.attn = CausalSelfAttention(dim, n_heads, dropout)
        self.ln2 = nn.LayerNorm(dim)
        self.mlp = nn.Sequential(
            nn.Linear(dim, d_ff),
            nn.GELU(),
            nn.Linear(d_ff, dim),
            nn.Dropout(dropout),
        )

    def forward(self, x):
        x = x + self.attn(self.ln1(x))
        x = x + self.mlp(self.ln2(x))
        return x

# -------------------------
# Tiny GPT-like model
# -------------------------
class TinyGPT(nn.Module):
    def __init__(self, vocab_size, max_seq_len, d_model, n_heads, n_layers, d_ff, dropout):
        super().__init__()
        self.tok_emb = nn.Embedding(vocab_size, d_model)
        self.pos_emb = PositionalEmbedding(max_seq_len, d_model)
        self.drop = nn.Dropout(dropout)
        self.blocks = nn.ModuleList([
            TransformerBlock(d_model, n_heads, d_ff, dropout) for _ in range(n_layers)
        ])
        self.ln_f = nn.LayerNorm(d_model)
        self.head = nn.Linear(d_model, vocab_size, bias=False)

    def forward(self, idx):
        # idx: (B, T) token ids
        B, T = idx.size()
        tok_embeddings = self.tok_emb(idx)                  # (B, T, D)
        pos_embeddings = self.pos_emb(T).expand(B, -1, -1)  # (B, T, D)
        x = self.drop(tok_embeddings + pos_embeddings)
        for block in self.blocks:
            x = block(x)
        x = self.ln_f(x)
        logits = self.head(x)  # (B, T, vocab_size)
        return logits

# -------------------------
# Toy training loop for demonstration (not for real scale)
# -------------------------
def demo_train_step():
    model = TinyGPT(vocab_size, max_seq_len, d_model, n_heads, n_layers, d_ff, dropout).to(device)
    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)
    # toy data: random token sequences (replace with tokenizer & real data)
    batch_size = 16
    seq_len = 32
    x = torch.randint(0, vocab_size, (batch_size, seq_len), device=device)
    # target is next-token prediction: shift left by 1
    targets = x.clone()
    logits = model(x)  # (B, T, V)
    loss = F.cross_entropy(logits.view(-1, vocab_size), targets.view(-1))
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    print(f"toy loss: {loss.item():.4f}")

if __name__ == "__main__":
    demo_train_step() 

